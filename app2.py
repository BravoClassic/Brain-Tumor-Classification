import streamlit as st
import tensorflow as tf
from tensorflow.keras.models import load_model
from tensorflow.keras.utils import load_img,img_to_array
import numpy as np
import plotly.graph_objects as go
import cv2
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Flatten
from tensorflow.keras.optimizers import Adamax
from tensorflow.keras.metrics import Precision, Recall
from PIL import Image
import seaborn as sns
from pyngrok import ngrok
from dotenv import load_dotenv
import os
import google.generativeai as genai
from tensorflow.keras.layers import Conv2D, MaxPooling2D
from tensorflow.keras import regularizers
import base64
from openai import OpenAI
from groq import Groq


load_dotenv()

genai.configure(api_key=os.environ.get('GOOGLE_API_KEY'))
# Recreate the model architecture (same as before)

output_dir = 'saliency_maps'
os.makedirs(output_dir, exist_ok=True)

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")


def prompt_text(model_prediction, confidence):
  prompt=f"""You are an expert neurologist. You are tasked with explaining a
  saliency map of a brain tumor MRI scan. The saliency map was generated by a deep
  learning model that was trained to classify brain tumors into four categories:
  glioma, meningioma, no tumor, and pituitary tumor.

  The saliency map highlights the regions of the image that the machine learning model
  is focusing on to make the prediction.

  The deep learning model predicted the image to be of class '{model_prediction}' with
  a confidence of {confidence*100}%.

  In your response:
  - Explain what regions of the brain the model is focusing on, based on the saliency map.
  Refer to the regions highlighted in light cyan, those are the regions where the model is
  focusing on.
  - Explain possible reasons why the model made the prediction it did.
  - Don't mention anything like 'The saliency map highlights the regions the model is focusing on,
  which are in light cyan' in your explanation.
  - Keep your explanations to 4 sentences max.
  Let's think step by step about this. Verify step by step your explanation.
  """


  return prompt


def generate_explanation(img_path, model_prediction, confidence,llm_model):
  prompt = prompt_text(model_prediction, confidence)

  img = Image.open(img_path)

  # Convert image to base64 encoding to be processed by Groq and Openai
  def encode_image(img_path):
      with open(img_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode('utf-8')
  base64_image = encode_image(img_path)



  if llm_model == "OpenAI":

    # Getting the base64 string
    client = OpenAI(
        api_key=OPENAI_API_KEY
    )
    response = client.chat.completions.create(
        model="gpt-4o-mini",
      messages=[
        {
          "role": "user",
          "content": [
            {
              "type": "text",
              "text": prompt,
            },
            {
              "type": "image_url",
              "image_url": {
                "url":  f"data:image/jpeg;base64,{base64_image}"
              },
            },
          ],
        }
      ],
    )
    return response.choices[0].message.content
  elif llm_model == "Google":
    model = genai.GenerativeModel(model_name='gemini-1.5-flash')
    response = model.generate_content([prompt,img])
    return response.text
  elif llm_model == "Llama Vision":
    client = Groq(api_key=os.environ.get("GROK_API_KEY"))

    response = client.chat.completions.create(
      model="llama-3.2-11b-vision-preview",
      messages=[
        {
          "role": "user",
          "content": [
            {
              "type": "text",
              "text": prompt,
            },
              {
              "type": "image_url",
              "image_url": {
                "url":  f"data:image/jpeg;base64,{base64_image}"
              },
            },


          ]

        }
      ],
    )
    return response.choices[0].message.content


def generate_saliency_map(model, img_array, class_index, img_size):
  with tf.GradientTape() as tape:
    img_tensor = tf.convert_to_tensor(img_array)
    tape.watch(img_tensor)
    predictions = model(img_tensor)
    target_class = predictions[:, class_index]

  gradients = tape.gradient(target_class, img_tensor)
  gradients = tf.math.abs(gradients)
  gradients = tf.reduce_max(gradients, axis=-1)
  gradients = gradients.numpy().squeeze()

  # Resize gradients to match original image size
  gradients = cv2.resize(gradients, img_size)

  # Create a circular mask for the brain area
  center = (gradients.shape[0]//2, gradients.shape[1]//2)
  radius = min(center[0], center[1]) - 10
  y,x = np.ogrid[:gradients.shape[0], :gradients.shape[1]]
  mask = (x - center[0])**2 + (y-center[1])**2 <= radius ** 2

  # Apply mask to gradients
  gradients = gradients * mask

  # Normalize only the brain area
  brain_gradients = gradients[mask]
  if brain_gradients.max() > brain_gradients.min():
    brain_gradients = (brain_gradients - brain_gradients.min()) / (brain_gradients.max()-brain_gradients.min())
  gradients[mask] = brain_gradients

  # Apoly a higher threshold
  threshold = np.percentile(gradients[mask], 80)
  gradients[gradients < threshold] = 0

  # Apply more aggressive smoothign
  gradients = cv2.GaussianBlur(gradients, (11,11), 0)

  # Create a heatmap overlay with enhanced contrast
  heatmap = cv2.applyColorMap(np.uint8(255*gradients), cv2.COLORMAP_JET)
  heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)

  # Resize heatmap to match original image size
  heatmap = cv2.resize(heatmap, img_size)

  # Superimpose the heatmap on original image with increase opacity
  original_img = img_to_array(img)
  superimposed_img = heatmap * 0.7 + original_img * 0.3
  superimposed_img = superimposed_img.astype(np.uint8)

  img_path = os.path.join(output_dir, uploaded_file.name)
  with open(img_path, 'wb') as f:
    f.write(uploaded_file.getbuffer())

  saliency_map_path = f'saliency_maps/{uploaded_file.name}'

  # Save the saliency map
  cv2.imwrite(saliency_map_path, cv2.cvtColor(superimposed_img,cv2.COLOR_RGB2BGR))

  return superimposed_img






def load_custom_cnn(model_path):

  cnn_model = Sequential()

  # Convolutional layers
  cnn_model.add(Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(224, 224, 3)))
  cnn_model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))
  cnn_model.add(MaxPooling2D(pool_size=(2, 2)))


  cnn_model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))
  cnn_model.add(Conv2D(256, (3, 3), padding='same', activation='relu'))
  cnn_model.add(MaxPooling2D(pool_size=(2, 2)))

  # Flatten the output for fully connected layers
  cnn_model.add(Flatten())

  # Fully connected layers
  cnn_model.add(Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.01)))
  cnn_model.add(Dropout(0.5))

  cnn_model.add(Dense(4, activation='softmax'))  # 4 classes with 4 neurons for output layer


  # Load the weights
  cnn_model.load_weights(model_path)
  return cnn_model





def load_xception_model(model_path):
  image_shape = (299,299,3)
  base_model = tf.keras.applications.Xception(include_top=False,weights="imagenet",
                                              input_shape=image_shape, pooling="max")
  model = Sequential([
      base_model,
      Flatten(),
      Dropout(rate=0.3),
      Dense(128, activation="relu"),
      Dropout(rate=0.25),
      Dense(4,activation="softmax")
  ])
  model.build((None,)+image_shape)

  # Compile the model
  model.compile(Adamax(learning_rate=0.001),loss="categorical_crossentropy",
                metrics=['accuracy',Precision(), Recall()])

  model.load_weights(model_path)
  return model

st.title('Brain Tumor Classification')
st.write('Upload an MRI image to classify the tumor')

st.subheader("Dataset Used")
st.code(f"""
kaggle datasets download -d masoudnickparvar/brain-tumor-mri-dataset --unzip
""")

uploaded_file = st.file_uploader("Choose an image...", type=["jpg","jpeg","png"])

if uploaded_file is not None:
  image = Image.open(uploaded_file)
  selected_model = st.radio(
      "Select Model",
      ("Transfer Learning - Xception", "Custom CNN")
  )
  img_size = (299,299) if selected_model == "Transfer Learning - Xception" else (224,224)
  if selected_model == "Transfer Learning - Xception":
    model = load_xception_model("xception_model.weights.h5")
    img_size = (299,299)
    print("Using Xception model for classification")
  elif selected_model == "Custom CNN":
    model = load_custom_cnn("cnn_model.weights.h5")
    img_size = (224,224)
    print("Using Custom CNN model for classification")
  label =['Giloma', 'Meningioma', 'No tumor', 'Pituitary']
  img = load_img(uploaded_file, target_size=img_size )
  img_array = img_to_array(img)
  img_array = np.expand_dims(img_array, axis=0) / 255.0
  prediction = model.predict(img_array)

  # Get the class with the highest probability
  class_index = np.argmax(prediction[0])
  predicted_class = label[class_index]

  # st.write(f"Predicted Class: {predicted_class}")
  # st.write("Predictions:")
  # for i, label in enumerate(label):
  #   st.write(f"{label}: {prediction[0][i]*100:.2f}%")

  st.header("Classification Results")
  col3, col4 = st.columns(2)
  col1, col2 = st.columns(2)
  col1.metric("Prediction", predicted_class)
  col2.metric("Confidece", f"{prediction[0][class_index]*100:.2f}%")

  saliency_map = generate_saliency_map(model, img_array, class_index, img_size)
  col1, col2 = st.columns(2)
  with col1:
    st.subheader("Saliency Map")
    st.image(saliency_map, caption='Saliency Map', use_container_width=True)
  with col2:
    st.subheader("Original Image")
    st.image(image, caption='Original Image', use_container_width=True)

  saliency_map_path = f'saliency_maps/{uploaded_file.name}'
  st.subheader("Explanation")
  llm = st.selectbox(
    "Select LLM",
    ("OpenAI", "Google", "Llama Vision")
  )
  if llm is not None:
    explanation = generate_explanation(saliency_map_path, predicted_class, prediction[0][class_index], llm)

  st.write(explanation)

  st.header("Chat with Brain Tumor Expert")
  client = OpenAI(api_key=os.environ.get('OPENAI_API_KEY') )

  # Set a default model
  if "openai_model" not in st.session_state:
      st.session_state["openai_model"] = "gpt-4o"

  if "previous_image" not in st.session_state or st.session_state.previous_image != uploaded_file.name :
        # Clear chat history
        st.session_state.messages = []
        # Update previous image
        st.session_state.previous_image = uploaded_file.name

  # Initialize chat history

  if "messages" not in st.session_state or st.session_state.messages == []:
      st.session_state.messages = [
          {"role": "user", "content": f"{prompt_text(predicted_class, prediction[0][class_index])}\n- Answer based on information provided and you are an expert neurologist."},
        {"role": "assistant", "content": explanation}

      ]

  # Display chat messages from history on app rerun

  for message in st.session_state.messages:
      with st.chat_message(message["role"]):
          st.markdown(message["content"])

  # Accept user input
  if prompt := st.chat_input("Answer any question about your condition?"):
      # Add user message to chat history
      st.session_state.messages.append({"role": "user", "content": str(prompt)})
      # Display user message in chat message container
      with st.chat_message("user"):
          st.markdown(prompt)
      # Display assistant response in chat message container
      with st.chat_message("assistant"):
        stream = client.chat.completions.create(
            model=st.session_state["openai_model"],
            messages=[
                {"role": m["role"], "content": m["content"]}
                for m in st.session_state.messages
            ],
            stream=True,
        )
        response = st.write_stream(stream)
      st.session_state.messages.append({"role": "assistant", "content": response})
